\documentclass{llncs}
\usepackage{makeidx}

\title{Symbolic Music Similarity}
\author{Ali Bektas \and Paul Kröger}
\date{April 2020}


\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage{verbatim}
\usepackage{mathtools}


%Eine Ausarbeitung hat einen Titel, einen (oder mehrere) Verfasser, einen Betreuer, ein
%Erstellungsdatum und ist in einem Seminar in einem Jahr verfasst worden. Geben Sie das
%alles auf einer Titelseite an. 
\title{Symbolic Melodic Music Similarity}
\titlerunning{Symbolic Music Similarity}
	\author{Ali Bektas \and Paul Kröger \\ \textbf{Betruer :} Prof. Dr. Ulf Leser \\ Similarity Search \\ WS19-20}
	%TODO : Man sollte hier noch das Datum einfügen.
	\date{\today}
  	\institute{ \vspace{5px} 
  	Humboldt Universität zu Berlin\\}
	


\begin{document}
	 
	\mainmatter

	
	\maketitle


	\begin{abstract}
	With an increasing need to classify musical data in terms of their content 
	the question of similarity between musical objects becomes a challange. While 
	there is not a single similarity function to answer all different needs , various approaches
	have been introduced to the field in recent decades. In our paper , we introduce the approaches
	which mainly focus on symbolic data , that is , a representation of the content based on an alphabet
	of symbols and syntax.
	\end{abstract}

	\section{Introduction}
		Similarity measures lie at the heart of Information Retrieval. This is also for Music Similarity the case. For a subscriber of a music streaming application like Spotify , Last.fm etc. to recommend latest albums or a musicologist to find related documents in a database , similarity measures are crucial. In comparison to its counterpart Audio Music Similarity , Symbolic Music Similarity algorithms use symbols that represent the data , where in Audio Music Similarity one has pitch-time related values. Within Symbolic Music Similarity we find algorithms that deal with harmony and those that deal with melody. What differs harmony from melody is that by harmony we see chords , that is multiple notes played at a discrete time $t$ that together form a unity.  

		Representation of music dates back to as early as 2000 BC \cite{kil:civ}. Representing music over an alphabet consists in describing the pitch and the rhythm. Since we are mainly concerned with Western music , we can restrict ourselves to 12 tones of an octave , other cultures however may use more or less notes. In Middle Eastern countries ,for instance , we see that there are 9 tones between two notes of an whole tone interval.

		In further sections we will introduce some algorithms which implement basic text-similarity related operations to conclude similarity between melodies. That being said , we will also introduce  algorithms , that treat musical data different than text-similarity algorithms would treat  strings. The MelodyShape Algorithm of J. Urbano \cite{five_point_two}  and its variations give the best results in MIREX's annual competitions ,an EXchange group for Music Information Retrieval , between years 201
		%TODO
		Urbano's MelodyShape forms a spline for the query melody and compares it with other splines to determine similarity between pieces. We will conclude from this , that the field evolved in recent years in such a way that the text-based approaches are less promising for the future of the field.

		Other than MelodyShape and its variations we will also present a graph-based approach \cite{two_point_four} , which incorporates routines to generalize the melody, that heavily depend on Music Theory , a geometrical approach which use polygonal chains that are built by projecting the melody onto a plain of pitch and duration.

		In our paper we will follow the classification used by Velardo et al. \cite{two}. According to Velardo et al. a melodic music similarity algorithm belongs to either of the four classes (1) Music Theory , (2) Cognition , (3) Mathematics , (4) Hybrid. Hybrid algorithms are usually formed by taking a linear combination of different similarity measures. 

		Our main emphases are that the field suffers from the subjectivity of music similarity and that one single algorithms fails to answer all needs , which is especially the case when algorithms are tested only against a narrow range of data. In order to reduce these drawbacks we will show how MIREX , an EXchange group for Music Information Retrieval , took statistical approaches to form a Ground Truth for the data , that were collected from experts' evaluations. 
	
	\section{Algorithms}

		\subsection{A Graph Based Approach}

		Orio et al. \cite{two_point_four} introduce in their paper a series of operations to reduce melodies into a single large tree. Melodies are segmentated and then these segments are added into the tree as terminal nodes. The intermediate nodes represent generalization of the segments. 
		In a single step of generalization a segment is transformed into a simpler segment by deleting less important notes in the given segment. Which notes are less important is decided by three coefficients : \textit{(1)} its underlying harmonic function , \textit{(2)} its metric position  and \textit{(3)} the interval between the tone and the root of the underlying chord . 
		In order to determine these coefficients , harmonic analysis must be applied. Harmonic analysis is the process of making statements about in which way notes of a sequence are related to each other. These three coefficents must then be determined in such a way that it reflects the priorities of the human when considering two songs to be similar. 
		In their paper Orio et al. state that they chose to manually annotate the functions of notes in order to prevent any problems that could have otherwise arised during automated annotation , which could cause wrong simplification steps. We see this as a serious challange for the data , that only contain melody in comparison to data with harmonic elements , which could give better results of the underlying harmony. 

		The distance between two documents is expressed as the median of shortest distance between all segments. The similarity function $s(c_i , q)$ between a document $c_i$ from a collection of $N$ documents and a query document q is calculated as :
		
		\begin{equation}
			s(c_i , q)= (1 + \frac{d(c_i , q)}{\sum_{j=1}^{N} \frac{d(c_i,c_j)}{N-1}}) 
		\end{equation}

		The similarity function is normalized and authors mention that 'the normalization factors can be computed off-line to speed up retrieval'. Authors also mention that the tree representation can offer novel ways to view a collection and see visually to what extent two songs were considered to be similar.

		\subsection{Polygonal Chains}

		\subsection{}


		
	\section{MIREX}
		MIREX is a platform for enthusiasts of this field to exchange ideas. It arranges annual competition where researchers present their algorithms. The subbranch "Symbolic Melodic Music Similarity" doesn't take place since 2016. 


		As we have mentioned in Introduction , one of the main problems of Symbolic Melodic Music Similarity is that there is no consensus over a universal measure of similarity. In order to circumvent this issue , MIREX consults ratings given by experts of the field. The results of the comptetitors were then compared against this so-called Ground Truth. As a new way to measure recall R. Typke introduces Average Dynamic Recall in his report \cite{three}.       


		\subsection{The Ground Truth}
 		The RISM A/II collection which was used as the collection on the competition in year 2005 contains 476,000. Experts were asked to rate similarity of documents in this collection to given queries. The experts were not asked all the documents , since it would take much time. A series of filtering processes were then applied to reduce the number of relevant documents. 

 		Among several of them documents were filtered based on: 

 		\begin{itemize}
 			\item The interval between the highest and lowest note in the incipit
 			\item The largest interval between subsequent notes.
 			\item The editing distance between the query and the document. In order to find the editing distance , a document is projected onto a string , that contains the alphabet U("up") , D("down") , R("repeat"). 
 		\end{itemize} 

 		Different filtering steps are used based on characteristic features of the documents. In order to prevent relevent documents from being filtered out , a limit of 300 documents were set. To come at a convenient number of documents , residual documents were then manually reduced to a collection of 50 documents.

 		The relevant documents were then given to the experts to be ordered. Experts were given the freedom to choose which documents were to include at all.  


		\subsection{Average Dynamic Recall}







	\section{Evaluation}
		Sth Not so boring tho.

	\section{Conclusion}
		Bluh bluh
		
	

	\begin{thebibliography}{5}
	
	\bibitem {kil:civ}
	A. D. Kilmer and M. Civil, 
	"Old Babylonian Musical Instructions Relating to Hymnody" 
	Journal of Cuneiform Studies 38, 
	no. 1 (Spring 1986): 94-98.

	\bibitem{five_point_two} 
	J. Urbano. MelodyShape at 
	MIREX 2014 Symbolic Melodic Similarity. 
	Technical report, Music Information Retrieval Evaluation eXchange, 2014.

	\bibitem{two}
	Symbolic Melodic Similarity: State of the Art and Future Challenges
	Valerio Velardo,Mauro Vallati and Steven Jan
	Computer Music Journal, 40:2, pp. 70–83, Summer 2016 doi:10.1162/COMJ a 00359
	2016 , Massachusetts Institute of Technology.v

	\bibitem{two_point_four} Orio, N., and A. Rodá. 2009. “A Measure of Melodic Similarity Based on a Graph Representation of the Music Structure.” In Proceedings of the International Conference for Music Information Retrieval, pp. 543– 548.

	\bibitem{three} Typke, Rainer. (2007). Music Retrieval based on Melodic Similarity.

	\bibitem{one} Greg Aloupis, Thomas Fevens, Stefan Langerman, Tomomi Matsui, Antonio Mesa, Yurai Nunez, David Rappaport, and Godfried Toussaint, "Algorithms for Computing Geometric Measures of Melodic Similarity" Computer Music Journal, Vol.30, No. 3 (Autumn, 2006), pp. 67-76

	\end{thebibliography}

	
\end{document}
